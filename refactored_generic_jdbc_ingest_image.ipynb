{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Widgets\n",
                "dbutils.widgets.text(\"jdbc_hostname\", \"\")\n",
                "dbutils.widgets.text(\"jdbc_port\", \"\")\n",
                "dbutils.widgets.text(\"jdbc_db\", \"\")\n",
                "dbutils.widgets.text(\"user_name\", \"\")\n",
                "dbutils.widgets.text(\"table_name\", \"\")\n",
                "dbutils.widgets.text(\"dest_catalog\", \"\")\n",
                "dbutils.widgets.text(\"scope\", \"\")\n",
                "dbutils.widgets.text(\"dest_schema\", \"\")\n",
                "dbutils.widgets.text(\"src_schema\", \"\")\n",
                "dbutils.widgets.text(\"db_type\", \"\")\n",
                "dbutils.widgets.text(\"overwrite\", \"\")\n",
                "dbutils.widgets.text(\"incremental_field\", \"\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Imports and logging setup\n",
                "import logging\n",
                "import typing as t\n",
                "from delta.tables import DeltaTable\n",
                "from pyspark.sql import DataFrame\n",
                "from pyspark.sql.utils import AnalysisException\n",
                "from pyspark.sql.functions import lower, collect_list, date_format\n",
                "\n",
                "logging.basicConfig(level=logging.INFO)\n",
                "logger = logging.getLogger(__name__)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Main execution\n",
                "db_type = dbutils.widgets.get(\"db_type\")\n",
                "jdbc_hostname = dbutils.widgets.get(\"jdbc_hostname\")\n",
                "jdbc_port = dbutils.widgets.get(\"jdbc_port\")\n",
                "jdbc_db = dbutils.widgets.get(\"jdbc_db\")\n",
                "user_name = dbutils.widgets.get(\"user_name\")\n",
                "scope = dbutils.widgets.get(\"scope\")\n",
                "password = dbutils.secrets.get(scope=scope, key=user_name)\n",
                "dest_catalog = dbutils.widgets.get(\"dest_catalog\")\n",
                "dest_schema = dbutils.widgets.get(\"dest_schema\")\n",
                "table_key_name = dbutils.widgets.get(\"table_key_name\")\n",
                "incremental_field = dbutils.widgets.get(\"incremental_field\")\n",
                "\n",
                "jdbc_url = get_jdbc_url(db_type, jdbc_hostname, jdbc_port, jdbc_db)\n",
                "jdbc_driver = get_jdbc_driver(db_type)\n",
                "connection_props = get_connection_properties(user_name, password, jdbc_driver)\n",
                "\n",
                "overwrite = dbutils.widgets.get(\"overwrite\").lower() == \"true\"\n",
                "\n",
                "try:\n",
                "    check_and_set_catalog_schema(dest_catalog, dest_schema)\n",
                "    process_table(\n",
                "        jdbc_url,\n",
                "        connection_props,\n",
                "        dest_catalog,\n",
                "        dest_schema,\n",
                "        dbutils.widgets.get(\"table_name\"),\n",
                "        dbutils.widgets.get(\"src_schema\"),\n",
                "        db_type,\n",
                "        overwrite,\n",
                "        table_key_name,\n",
                "        incremental_field,\n",
                "    )\n",
                "except ValueError as e:\n",
                "    logger.error(f\"Error setting up catalog and schema: {str(e)}\")\n",
                "    raise"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Helper functions\n",
                "def get_connection_properties(\n",
                "    user_name: str, \n",
                "    password: str, \n",
                "    driver: str\n",
                "    ) -> t.Dict[str, str]:\n",
                "    \"\"\"\n",
                "    Create a dictionary of JDBC connection properties.\n",
                "    \"\"\"\n",
                "    return {\n",
                "        \"user\": user_name,\n",
                "        \"password\": password,\n",
                "        \"driver\": driver,\n",
                "        \"trustServerCertificate\": \"true\",\n",
                "        \"num_partitions\": \"25\",\n",
                "    }\n",
                "\n",
                "\n",
                "def get_jdbc_url(\n",
                "    db_type: str, \n",
                "    hostname: str, \n",
                "    port: str, db: str\n",
                "    ) -> str:\n",
                "    \"\"\"\n",
                "    Generate a JDBC URL based on the database type and connection details.\n",
                "    \"\"\"\n",
                "    if db_type == \"sqlserver\":\n",
                "        return f\"jdbc:sqlserver://{hostname}:{port};database={db}\"\n",
                "    elif db_type == \"db2\":\n",
                "        return f\"jdbc:db2://{hostname}:{port}/{db}\"\n",
                "    else:\n",
                "        raise ValueError(f\"Unsupported database type: {db_type}\")\n",
                "\n",
                "\n",
                "def get_jdbc_driver(db_type: str) -> str:\n",
                "    \"\"\"\n",
                "    Get the JDBC driver class name for the specified database type.\n",
                "    \"\"\"\n",
                "    if db_type == \"sqlserver\":\n",
                "        return \"com.microsoft.sqlserver.jdbc.SQLServerDriver\"\n",
                "    elif db_type == \"db2\":\n",
                "        return \"com.ibm.db2.jcc.DB2Driver\"\n",
                "    else:\n",
                "        raise ValueError(f\"Unsupported database type: {db_type}\")\n",
                "\n",
                "\n",
                "def read_jdbc_data(\n",
                "    jdbc_url: str, \n",
                "    table_name: str, \n",
                "    connection_props: t.Dict[str, str]\n",
                "    ) -> DataFrame:\n",
                "    \"\"\"\n",
                "    Read data from a JDBC source into a Spark DataFrame.\n",
                "    \"\"\"\n",
                "    return spark.read.jdbc(url=jdbc_url, table=table_name, properties=connection_props)\n",
                "\n",
                "\n",
                "def remove_spaces_from_column_headers(df: DataFrame) -> DataFrame:\n",
                "    \"\"\"\n",
                "    Remove spaces and parentheses from column names in a Spark DataFrame.\n",
                "    \"\"\"\n",
                "    for col_name in df.columns:\n",
                "        new_col_name = col_name.replace(\" \", \"\").replace(\"(\", \"\").replace(\")\", \"\")\n",
                "        df = df.withColumnRenamed(col_name, new_col_name)\n",
                "    return df\n",
                "\n",
                "\n",
                "def get_primary_keys(\n",
                "    jdbc_url: str,\n",
                "    schema: str,\n",
                "    table_name: str,\n",
                "    connection_props: t.Dict[str, str],\n",
                "    db_type: str,\n",
                "    ) -> t.Dict[str, t.List[str]]:\n",
                "    \"\"\"\n",
                "    Retrieve primary key information for a given table.\n",
                "    \"\"\"\n",
                "    if db_type == \"sqlserver\":\n",
                "        query = f\"\"\"\n",
                "        (SELECT kcu.TABLE_NAME, kcu.COLUMN_NAME \n",
                "        FROM INFORMATION_SCHEMA.TABLE_CONSTRAINTS AS tc\n",
                "        JOIN INFORMATION_SCHEMA.KEY_COLUMN_USAGE AS kcu\n",
                "        ON tc.CONSTRAINT_TYPE = 'PRIMARY KEY' AND tc.CONSTRAINT_NAME = kcu.CONSTRAINT_NAME AND tc.TABLE_NAME = kcu.TABLE_NAME\n",
                "        WHERE tc.TABLE_SCHEMA = '{schema}' AND tc.TABLE_NAME = '{table_name}') AS primary_key_info\n",
                "        \"\"\"\n",
                "        df = read_jdbc_data(jdbc_url, query, connection_props)\n",
                "        result = (\n",
                "            df.groupBy(lower(df.TABLE_NAME).alias(\"table_name\"))\n",
                "            .agg(collect_list(\"COLUMN_NAME\").alias(\"column_names\"))\n",
                "            .collect()\n",
                "        )\n",
                "        return {row[\"table_name\"]: row[\"column_names\"] for row in result}\n",
                "\n",
                "    elif db_type == \"db2\":\n",
                "        query = f\"\"\"\n",
                "        (WITH table_index_p AS (\n",
                "        SELECT\n",
                "            si.tbname,\n",
                "            RTRIM(si.name) AS index_name,\n",
                "            RTRIM(si.creator) AS index_schema,\n",
                "            si.uniquerule,\n",
                "            si.colnames\n",
                "        FROM sysibm.sysindexes si\n",
                "        WHERE lower(si.tbname) = '{table_name.lower()}'\n",
                "        AND lower(si.tbcreator) = '{schema.lower()}'\n",
                "        AND si.uniquerule = 'P'\n",
                "        ),\n",
                "        table_index_d AS (\n",
                "        SELECT\n",
                "            si.tbname,\n",
                "            RTRIM(si.name) AS index_name,\n",
                "            RTRIM(si.creator) AS index_schema,\n",
                "            si.uniquerule,\n",
                "            si.colnames\n",
                "        FROM sysibm.sysindexes si\n",
                "        LEFT JOIN table_index_p tip ON rtrim(si.tbname) = tip.tbname \n",
                "        WHERE lower(si.tbname) = lower('{table_name}')\n",
                "        AND lower(si.tbcreator) = lower('{schema}')\n",
                "        AND si.uniquerule = 'D'\n",
                "        and tip.tbname IS NULL \n",
                "        )\n",
                "        SELECT tbname as TABLE_NAME, replace(substr(colnames,2),'+',',') AS COLUMN_NAME FROM table_index_p \n",
                "        UNION \n",
                "        SELECT tbname as TABLE_NAME, replace(substr(colnames,2),'+',',') AS COLUMN_NAME FROM table_index_d ) AS primary_key_info\n",
                "        \"\"\"\n",
                "        df = read_jdbc_data(jdbc_url, query, connection_props)\n",
                "        pk_list = (\n",
                "            df.select(\"COLUMN_NAME\")\n",
                "            .rdd.flatMap(lambda x: x[0].split(\",\"))\n",
                "            .distinct()\n",
                "            .collect()\n",
                "        )\n",
                "        table_name_src = f\"{schema.lower()}.{table_name.lower()}\"\n",
                "        return {table_name_src: pk_list}\n",
                "\n",
                "    else:\n",
                "        raise ValueError(f\"Unsupported database type: {db_type}\")\n",
                "\n",
                "\n",
                "def check_table_exists(\n",
                "    catalog: str, \n",
                "    schema: str, \n",
                "    table_name: str\n",
                "    ) -> bool:\n",
                "    \"\"\"\n",
                "    Check if a table exists in the specified catalog and schema.\n",
                "    \"\"\"\n",
                "    full_table_name = f\"{catalog}.{schema}.{table_name}\"\n",
                "    try:\n",
                "        # Method 1: Using spark.catalog.tableExists\n",
                "        return spark.catalog.tableExists(full_table_name)\n",
                "    except Exception as e:\n",
                "        logger.warning(f\"tableExists method failed for {full_table_name}: {str(e)}. Trying alternative method.\")\n",
                "    \n",
                "    try:\n",
                "        # Method 2: Using SHOW TABLES SQL command\n",
                "        df = spark.sql(f\"SHOW TABLES IN {catalog}.{schema} LIKE '{table_name}'\")\n",
                "        return df.filter(df.tableName == table_name).count() > 0\n",
                "    except AnalysisException as e:\n",
                "        logger.error(f\"Error checking existence of table {full_table_name}: {str(e)}\")\n",
                "        return False\n",
                "\n",
                "\n",
                "def write_delta_table(\n",
                "    df: DataFrame,\n",
                "    mode: str,\n",
                "    catalog: str,\n",
                "    schema: str,\n",
                "    table: str,\n",
                "    partition_by: t.Optional[t.List[str]] = None,\n",
                "    ):\n",
                "    \"\"\"\n",
                "    Write a Spark DataFrame to a Delta table.\n",
                "    \"\"\"\n",
                "    writer = df.write.mode(mode).option(\"inferSchema\", \"true\").format(\"delta\")\n",
                "    if partition_by:\n",
                "        writer = writer.partitionBy(*partition_by)\n",
                "    writer.saveAsTable(f\"{catalog}.{schema}.{table}\")\n",
                "\n",
                "\n",
                "def merge_delta_table(\n",
                "    target_df: DeltaTable,\n",
                "    source_df: DataFrame,\n",
                "    merge_condition: str,\n",
                "    update_columns: t.Dict[str, str],\n",
                "    ):\n",
                "    \"\"\"\n",
                "    Merge a source DataFrame into a target Delta table.\n",
                "    \"\"\"\n",
                "    return (\n",
                "        target_df.alias(\"target\")\n",
                "        .merge(source_df.alias(\"source\"), merge_condition)\n",
                "        .whenMatchedUpdate(set=update_columns)\n",
                "        .whenNotMatchedInsert(\n",
                "            values={col: f\"source.{col}\" for col in source_df.columns}\n",
                "        )\n",
                "        .execute()\n",
                "    )\n",
                "\n",
                "\n",
                "def catalog_exists(catalog: str) -> bool:\n",
                "    \"\"\"\n",
                "    Check if a catalog exists.\n",
                "    \"\"\"\n",
                "    try:\n",
                "        df = spark.sql(\"SHOW CATALOGS\")\n",
                "        return df.filter(df.catalog == catalog).count() > 0\n",
                "    except AnalysisException as e:\n",
                "        logger.error(f\"Error checking catalog {catalog}: {str(e)}\")\n",
                "        return False\n",
                "\n",
                "\n",
                "def schema_exists(\n",
                "    catalog: str, \n",
                "    schema: str\n",
                "    ) -> bool:\n",
                "    \"\"\"\n",
                "    Check if a schema exists in the specified catalog.\n",
                "    \"\"\"\n",
                "    try:\n",
                "        df = spark.sql(f\"SHOW SCHEMAS IN {catalog}\")\n",
                "        return df.filter(df.databaseName == schema).count() > 0\n",
                "    except AnalysisException as e:\n",
                "        logger.error(f\"Error checking schema {schema} in catalog {catalog}: {str(e)}\")\n",
                "        return False\n",
                "\n",
                "\n",
                "def check_and_set_catalog_schema(\n",
                "    catalog: str, \n",
                "    schema: str\n",
                "    ) -> None:\n",
                "    \"\"\"\n",
                "    Check if the catalog and schema exist, and set them as the current catalog and schema.\n",
                "    \"\"\"\n",
                "    if not catalog_exists(catalog):\n",
                "        raise ValueError(f\"The catalog '{catalog}' does not exist.\")\n",
                "    logger.info(f\"The catalog '{catalog}' exists.\")\n",
                "\n",
                "    if not schema_exists(catalog, schema):\n",
                "        raise ValueError(\n",
                "            f\"The schema '{schema}' does not exist in catalog '{catalog}'.\"\n",
                "        )\n",
                "    logger.info(f\"The schema '{schema}' exists in catalog '{catalog}'.\")\n",
                "\n",
                "    spark.sql(f\"USE {catalog}.{schema}\")\n",
                "    logger.info(f\"Now using {catalog}.{schema}\")\n",
                "\n",
                "\n",
                "def create_merge_condition(pk_columns: t.List[str]) -> str:\n",
                "    \"\"\"\n",
                "    Create a merge condition string based on primary key columns.\n",
                "    \"\"\"\n",
                "    return \" AND \".join([f\"target.{col} = source.{col}\" for col in pk_columns])\n",
                "\n",
                "\n",
                "def create_update_columns(\n",
                "    source_df: DataFrame, \n",
                "    pk_columns: t.List[str]\n",
                "    ) -> t.Dict[str, str]:\n",
                "    \"\"\"\n",
                "    Create a dictionary of columns to update, excluding primary key columns.\n",
                "    \"\"\"\n",
                "    return {\n",
                "        f\"target.{col}\": f\"source.{col}\"\n",
                "        for col in source_df.columns\n",
                "        if col not in pk_columns\n",
                "    }\n",
                "\n",
                "\n",
                "def process_table(\n",
                "    jdbc_url: str,\n",
                "    connection_props: t.Dict[str, str],\n",
                "    dest_catalog: str,\n",
                "    dest_schema: str,\n",
                "    table_name: str,\n",
                "    src_schema: str,\n",
                "    db_type: str,\n",
                "    overwrite: bool,\n",
                "    table_key_name: str,\n",
                "    incremental_field: str,\n",
                "    ):\n",
                "    \"\"\"\n",
                "    Process a table by either creating, overwriting, or incrementally updating it in the destination.\n",
                "    This function handles different scenarios:\n",
                "    1. Full load (overwrite or new table)\n",
                "    2. Incremental update (if incremental_field is specified)\n",
                "    3. Full merge (if no incremental_field and not overwriting)\n",
                "    \"\"\"\n",
                "\n",
                "    logger.info(f\"Processing table: {table_name}\")\n",
                "    table_name_src = f\"{src_schema.lower()}.{table_name}\"\n",
                "    target_table_name = f\"{dest_catalog}.{dest_schema}.{table_name.lower()}\"\n",
                "    table_exists = check_table_exists(dest_catalog, dest_schema, table_name.lower())\n",
                "    if not table_exists or overwrite:\n",
                "        source_df = read_jdbc_data(jdbc_url, table_name_src, connection_props)\n",
                "        source_df = remove_spaces_from_column_headers(source_df)\n",
                "        write_delta_table(\n",
                "            source_df, \"overwrite\", dest_catalog, dest_schema, table_name.lower()\n",
                "        )\n",
                "        logger.info(f\"Table '{target_table_name}' has been created/overwritten.\")\n",
                "    else:\n",
                "        if incremental_field:\n",
                "            max_date_df = spark.sql(\n",
                "                f\"SELECT timestamp FROM (DESCRIBE HISTORY {target_table_name} LIMIT 1)\"\n",
                "            )\n",
                "            if max_date_df.count() > 0:\n",
                "                max_date = max_date_df.collect()[0][\"timestamp\"].strftime(\"%Y-%m-%d\")\n",
                "                query = f\"(SELECT * FROM {table_name_src} WHERE CAST({incremental_field} AS DATE) > CAST('{max_date}' AS DATE))\"\n",
                "            else:\n",
                "                query = table_name_src\n",
                "        else:\n",
                "            query = table_name_src\n",
                "        source_df = spark.read.jdbc(url=jdbc_url, table=query, properties=connection_props)\n",
                "        source_df = remove_spaces_from_column_headers(source_df)\n",
                "        if source_df.count() > 0:\n",
                "            target_df = DeltaTable.forName(spark, target_table_name)\n",
                "            pk_columns = [table_key_name] if table_key_name else get_primary_keys(\n",
                "                jdbc_url, src_schema, table_name, connection_props, db_type\n",
                "            )\n",
                "            if not pk_columns:\n",
                "                raise ValueError(\n",
                "                    f\"No Primary Key found for {table_name}, please specify key.\"\n",
                "                )\n",
                "            merge_condition = create_merge_condition(pk_columns)\n",
                "            update_columns = create_update_columns(source_df, pk_columns)\n",
                "            merge_delta_table(target_df, source_df, merge_condition, update_columns)\n",
                "            log_message = \"incrementally updated\" if incremental_field else \"fully merged\"\n",
                "            logger.info(f\"Table '{target_table_name}' has been {log_message}.\")\n",
                "        else:\n",
                "            logger.info(f\"No new data to update in '{target_table_name}'.\")\n",
                "    "
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.7"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
